{
  "name": "bos-robotstxt-generator",
  "version": "0.1.0",
  "private": true,
  "description": "Generate a clean, standardsâ€‘compliant robots.txt file for your site. This tool automates crawler rules, indexing preferences, and sitemap references, making it easy to manage search engine behavior and strengthen your site's technical SEO.",
  "author": "Blackout Secure <https://github.com/blackoutsecure>",
  "main": "dist/index.js",
  "license": "Apache-2.0",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/blackoutsecure/bos-robotstxt-generator.git"
  },
  "bugs": {
    "url": "https://github.com/blackoutsecure/bos-robotstxt-generator/issues"
  },
  "homepage": "https://github.com/blackoutsecure/bos-robotstxt-generator#readme",
  "keywords": [
    "github-action",
    "robots",
    "robots-txt",
    "crawling",
    "seo",
    "static-site",
    "automation"
  ],
  "scripts": {
    "build": "ncc build src/index.js -o dist",
    "clean": "node test/cli.js clean",
    "lint": "eslint . --fix",
    "format": "prettier --write \"**/*.{js,json,md}\"",
    "test": "mocha",
    "test:watch": "mocha --watch",
    "coverage": "nyc --reporter=html --reporter=text npm test",
    "coverage:ci": "nyc --reporter=lcov npm test",
    "pretest": "npm run build",
    "check": "eslint . && prettier --check \"**/*.{js,json,md}\" && npm test",
    "release": "bash .scripts/release.sh"
  },
  "dependencies": {
    "@actions/artifact": "^2.1.2",
    "@actions/core": "^1.11.1"
  },
  "devDependencies": {
    "@eslint/js": "^9.39.1",
    "@vercel/ncc": "^0.38.1",
    "eslint": "^9.12.0",
    "mocha": "^10.8.2",
    "nyc": "^17.1.0",
    "prettier": "^3.3.3"
  },
  "engines": {
    "node": ">=20"
  },
  "nyc": {
    "exclude": [
      "test/**",
      "dist/**",
      "node_modules/**",
      "eslint.config.js"
    ],
    "reporter": [
      "text",
      "html",
      "lcov"
    ],
    "all": true,
    "check-coverage": false,
    "branches": 70,
    "lines": 70,
    "functions": 70,
    "statements": 70
  }
}
